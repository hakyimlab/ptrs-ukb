{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "The main purpose of this notebook is to develope the code to read in phenotypes in desired format.\n",
    "At the end, I should wrap it up as the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 2.4.1\n",
      "SparkUI available at http://nucleus.cels.anl.gov:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.28-61941242c15d\n",
      "LOGGING: writing to /vol/bmd/yanyul/GitHub/ptrs-ukb/notebook/hail-20191213-1423-0.2.28-61941242c15d.log\n"
     ]
    }
   ],
   "source": [
    "import hail as hl\n",
    "hl.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-13 14:23:39 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-12-13 14:23:39 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-12-13 14:23:39 Hail: INFO: Number of variants across all BGEN files: 1255683\n"
     ]
    }
   ],
   "source": [
    "# just to load chr22 for testing purpose\n",
    "mt = hl.import_bgen(\n",
    "    '/vol/bmd/data/ukbiobank/genotypes/v3/ukb_imp_chr22_v3.bgen',\n",
    "    entry_fields = ['dosage'],\n",
    "    index_file_map = {'/vol/bmd/data/ukbiobank/genotypes/v3/ukb_imp_chr22_v3.bgen' : '/vol/bmd/yanyul/UKB/bgen_idx/ukb_imp_chr22_v3.bgen.idx2'},\n",
    "    sample_file = '/vol/bmd/data/ukbiobank/genotypes/v3/ukb19526_imp_chr1_v3_s487395.sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.annotate_cols(eid = mt.s.replace(\"\\_\\d+\", \"\"))\n",
    "mt = mt.key_cols_by('eid')\n",
    "mt = mt.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar_names = 'age_recruitment,sex,pc1,pc2'\n",
    "pheno_names = 'ht,mcv,mch'\n",
    "indiv_id = 'eid'\n",
    "int_names = 'age_recruitment,sex'\n",
    "str_names = 'eid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0, '../code/')\n",
    "from importlib import reload \n",
    "\n",
    "import my_hail_helper as myhelper\n",
    "\n",
    "myhelper = reload(myhelper)\n",
    "\n",
    "# Copied to ../code/my_hail_helper.py\n",
    "# def names_to_list(names):\n",
    "#     return names.split(',')\n",
    "# def get_dtype_dic(int_names, str_names, all_names):\n",
    "#     out_dic = { a : np.float for a in all_names }\n",
    "#     for i in int_names:\n",
    "#         out_dic[i] = np.int\n",
    "#     for i in str_names:\n",
    "#         out_dic[i] = str\n",
    "#     return out_dic\n",
    "# def read_and_split_phenotype_csv(csv_path, pheno_names, covar_names, indiv_id, int_names, str_names):\n",
    "#     # read in the phenotype table prepared from ukbREST output (in CSV format)\n",
    "#     # split and return two tables\n",
    "#     # 1. covariate table\n",
    "#     # 2. trait table\n",
    "#     covar_list = names_to_list(covar_names)\n",
    "#     pheno_list = names_to_list(pheno_names)\n",
    "#     all_colnames = covar_list + pheno_list + names_to_list(indiv_id)\n",
    "#     type_dic = get_dtype_dic(names_to_list(int_names), names_to_list(str_names), all_colnames)\n",
    "#     pheno = pd.read_csv(csv_path, dtype = type_dic, usecols = list(type_dic.keys())).rename(columns = {indiv_id : 's'})\n",
    "#     covar_table = pheno[covar_list + ['s']]\n",
    "#     trait_table = pheno[pheno_list + ['s']]\n",
    "#     return covar_table, trait_table\n",
    "# def read_indiv_list(file_path):\n",
    "#     # read in the file from file_path\n",
    "#     # and return the first columns as a list of string \n",
    "#     indiv_list = pd.read_csv(file_path, sep = '\\s+', dtype = str)\n",
    "#     return indiv_list.iloc[:,0].to_list()\n",
    "# def subset_by_col(df, colname, target_list):\n",
    "#     return df[df[colname].isin(target_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar, trait = myhelper.read_and_split_phenotype_csv(\n",
    "    '../output/query_phenotypes_cleaned_up.csv',\n",
    "    pheno_names = pheno_names,\n",
    "    covar_names = covar_names,\n",
    "    indiv_id = indiv_id,\n",
    "    int_names = int_names,\n",
    "    str_names = str_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we've loaded in the full covariate and trait tables\n",
    "Here we start to loop over all subsets and build the \"list of lists\" for traits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-13 14:23:57 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-13 14:24:09 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    }
   ],
   "source": [
    "subset_dic = {}\n",
    "nsubset = 2\n",
    "for subset_idx in range(1, nsubset + 1):\n",
    "    subset_indiv_list = myhelper.read_indiv_list(f'../output/data_split/British-training-{subset_idx}.txt')\n",
    "    sub_trait = myhelper.subset_by_col(trait, 'eid', subset_indiv_list)\n",
    "    sub_trait = myhelper.df_to_ht(sub_trait, 'eid')  # hl.Table.from_pandas(sub_trait, key = 's')\n",
    "#     sub_trait = sub_trait.repartition(40)\n",
    "    subset_dic[f'subset_{subset_idx}'] = sub_trait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-13 14:24:24 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    }
   ],
   "source": [
    "covar = myhelper.df_to_ht(covar, 'eid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Column fields:\n",
      "    's': str\n",
      "    'eid': str\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh37>\n",
      "    'alleles': array<str>\n",
      "    'rsid': str\n",
      "    'varid': str\n",
      "----------------------------------------\n",
      "Entry fields:\n",
      "    'dosage': float64\n",
      "----------------------------------------\n",
      "Column key: ['eid']\n",
      "Row key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_expr_ = {\n",
    "    k : subset_dic[k][mt.eid] for k in list(subset_dic.keys())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.annotate_cols(**annot_expr_)\n",
    "mt = mt.annotate_cols(covariates = covar[mt.eid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare trait and covar into list or list of lists\n",
    "subset_list = [ [ mt[f'subset_{i}'][j] for j in mt[f'subset_{i}'] ] for i in range(1, nsubset + 1) ]\n",
    "subset_names = [ [ f'subset_{i}_x_{j}' for j in mt[f'subset_{i}'] ] for i in range(1, nsubset + 1) ]\n",
    "covar_list = [ mt.covariates[i] for i in list(mt.covariates.keys()) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-13 15:03:41 Hail: WARN: 140933 of 487409 samples have a missing phenotype or covariate.\n",
      "2019-12-13 15:03:42 Hail: WARN: 140933 of 487409 samples have a missing phenotype or covariate.\n",
      "2019-12-13 15:03:42 Hail: INFO: linear_regression_rows[0]: running on 346476 samples for 3 response variables y,\n",
      "    with input variable x, and 5 additional covariates...\n",
      "2019-12-13 15:03:42 Hail: INFO: linear_regression_rows[1]: running on 346476 samples for 3 response variables y,\n",
      "    with input variable x, and 5 additional covariates...\n"
     ]
    }
   ],
   "source": [
    "gwas_out = hl.linear_regression_rows(\n",
    "    y = subset_list,\n",
    "    x = mt.dosage,\n",
    "    covariates = [1] + covar_list,\n",
    "    pass_through = ['varid', 'rsid']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    'phenotypes': array<array<str>> \n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh37> \n",
      "    'alleles': array<str> \n",
      "    'varid': str \n",
      "    'rsid': str \n",
      "    'n': array<int32> \n",
      "    'sum_x': array<float64> \n",
      "    'y_transpose_x': array<array<float64>> \n",
      "    'beta': array<array<float64>> \n",
      "    'standard_error': array<array<float64>> \n",
      "    't_stat': array<array<float64>> \n",
      "    'p_value': array<array<float64>> \n",
      "----------------------------------------\n",
      "Key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "gwas_out = gwas_out.annotate_globals(phenotypes = subset_names)\n",
    "gwas_out.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-13 16:53:21 Hail: INFO: Coerced sorted dataset\n",
      "2019-12-13 19:24:55 Hail: INFO: merging 40 files totalling 129.7M...\n",
      "2019-12-13 19:24:56 Hail: INFO: while writing:\n",
      "    test_output/gwas_subset_1_x_ht.tsv\n",
      "  merge time: 1.440s\n",
      "2019-12-13 19:24:58 Hail: INFO: merging 40 files totalling 129.7M...\n",
      "2019-12-13 19:25:00 Hail: INFO: while writing:\n",
      "    test_output/gwas_subset_1_x_mcv.tsv\n",
      "  merge time: 1.475s\n",
      "2019-12-13 19:25:02 Hail: INFO: merging 40 files totalling 129.7M...\n",
      "2019-12-13 19:25:05 Hail: INFO: while writing:\n",
      "    test_output/gwas_subset_1_x_mch.tsv\n",
      "  merge time: 3.062s\n",
      "2019-12-13 19:25:07 Hail: INFO: merging 40 files totalling 129.7M...\n",
      "2019-12-13 19:25:09 Hail: INFO: while writing:\n",
      "    test_output/gwas_subset_2_x_ht.tsv\n",
      "  merge time: 1.828s\n",
      "2019-12-13 19:25:11 Hail: INFO: merging 40 files totalling 129.7M...\n",
      "2019-12-13 19:25:14 Hail: INFO: while writing:\n",
      "    test_output/gwas_subset_2_x_mcv.tsv\n",
      "  merge time: 3.071s\n",
      "2019-12-13 19:25:16 Hail: INFO: merging 40 files totalling 129.7M...\n",
      "2019-12-13 19:25:18 Hail: INFO: while writing:\n",
      "    test_output/gwas_subset_2_x_mch.tsv\n",
      "  merge time: 1.433s\n"
     ]
    }
   ],
   "source": [
    "gwas_out = gwas_out.annotate( \n",
    "    variant = hl.delimit(\n",
    "        hl.array([\n",
    "            gwas_out['locus'].contig,\n",
    "            hl.str(gwas_out['locus'].position),\n",
    "            gwas_out['alleles'][0],\n",
    "            gwas_out['alleles'][1]\n",
    "        ]), \n",
    "    delimiter = ':')\n",
    ")\n",
    "gwas_out = gwas_out.key_by('variant')\n",
    "## Hey, this repartition is important\n",
    "## in the sense that it avoids the unnecessary and repeated sorting caused by key_by\n",
    "gwas_out = gwas_out.repartition(40)\n",
    "gwas_out = gwas_out.cache()\n",
    "phenotypes = gwas_out['phenotypes'].collect()[0]\n",
    "for i, subset in enumerate(phenotypes):\n",
    "    for j, trait in enumerate(subset):\n",
    "        ht_export = myhelper.gwas_formater_from_neale_lab(gwas_out, i, j)\n",
    "        ht_export.export(f'test_output/gwas_{trait}.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
