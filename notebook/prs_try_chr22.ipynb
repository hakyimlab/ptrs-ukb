{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 2.4.1\n",
      "SparkUI available at http://nucleus.cels.anl.gov:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.28-61941242c15d\n",
      "LOGGING: writing to /vol/bmd/yanyul/GitHub/ptrs-ukb/notebook/hail-20191224-0849-0.2.28-61941242c15d.log\n"
     ]
    }
   ],
   "source": [
    "import hail as hl\n",
    "hl.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-23 16:29:43 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "  Loading column 'minor_allele' as type 'str' (type not specified)\n",
      "  Loading column 'minor_AF' as type 'str' (type not specified)\n",
      "  Loading column 'low_confidence_variant' as type 'str' (type not specified)\n",
      "  Loading column 'n_complete_samples' as type 'str' (type not specified)\n",
      "  Loading column 'AC' as type 'str' (type not specified)\n",
      "  Loading column 'ytx' as type 'str' (type not specified)\n",
      "  Loading column 'beta' as type 'str' (type not specified)\n",
      "  Loading column 'se' as type 'str' (type not specified)\n",
      "  Loading column 'tstat' as type 'str' (type not specified)\n",
      "  Loading column 'pval' as type 'str' (type not specified)\n",
      "  Loading column 'rsid' as type 'str' (type not specified)\n",
      "  Loading column 'ref' as type 'str' (type not specified)\n",
      "  Loading column 'alt' as type 'str' (type not specified)\n",
      "\n",
      "2019-12-23 16:29:44 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'f0' as type 'str' (type not specified)\n",
      "\n",
      "2019-12-23 16:29:45 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-12-23 16:29:45 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-12-23 16:29:45 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-12-23 16:29:48 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-12-23 16:29:48 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-12-23 16:29:48 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-12-23 16:30:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-23 16:31:06 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-23 16:40:21 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    }
   ],
   "source": [
    "gwas_sample = hl.import_table(\n",
    "    '/vol/bmd/yanyul/UKB/gwas_on_subset/gwas_runs_in_tsv/gwas_in_tsv_subset13_x_height.tsv',\n",
    "    key = ['rsid', 'variant']\n",
    ")\n",
    "f = pd.read_csv('../external_data/martin_et_al_2019ng_table_s6_trait_description.tsv', sep = '\\t')\n",
    "traits = [ i.lower() for i in f['short'].to_list() ]\n",
    "clumped_snp_files = []\n",
    "for i in traits:\n",
    "    for j in range(1, 18):\n",
    "        clumped_snp_files.append('/vol/bmd/yanyul/UKB/ld_clump/gwas_clump_x_subset' + str(j) + '_x_' + i + '.clumped_snp')\n",
    "ht_var = hl.import_table(clumped_snp_files, no_header = True, key = 'f0')\n",
    "ht_var = ht_var.key_by('f0')\n",
    "gwas_sample = gwas_sample.annotate(is_clump_var = ht_var[gwas_sample.rsid])\n",
    "ht_var = gwas_sample.filter(hl.is_defined(gwas_sample.is_clump_var))\n",
    "chr_all = '{' + ','.join([str(i) for i in range(22, 23)]) + '}'\n",
    "index_dic = {\n",
    "    f'/vol/bmd/data/ukbiobank/genotypes/v3/ukb_imp_chr{i}_v3.bgen' : f'/vol/bmd/yanyul/UKB/bgen_idx/ukb_imp_chr{i}_v3.bgen.idx2' for i in range(1, 23)\n",
    "}\n",
    "mt = hl.import_bgen(\n",
    "    '/vol/bmd/data/ukbiobank/genotypes/v3/ukb_imp_chr' + chr_all + '_v3.bgen', \n",
    "    index_file_map = index_dic,\n",
    "    entry_fields = ['dosage'],\n",
    "    sample_file = '/vol/bmd/data/ukbiobank/genotypes/v3/ukb19526_imp_chr1_v3_s487395.sample',\n",
    "    variants = hl.parse_variant(ht_var.variant)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mt.count_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_files = []\n",
    "for i in ['African', 'Chinese', 'Indian']:\n",
    "    indiv_files.append('/vol/bmd/yanyul/GitHub/ptrs-ukb/output/data_split/' + i + '.txt')\n",
    "\n",
    "# for subset i\n",
    "# for trait height\n",
    "i = 1\n",
    "trait = 'height'\n",
    "\n",
    "for prefix in [ 'British-test-', 'British-validation-' ]:\n",
    "    indiv_files.append('/vol/bmd/yanyul/GitHub/ptrs-ukb/output/data_split/' + prefix + str(i) + '.txt')\n",
    "    indiv_files.append('/vol/bmd/yanyul/GitHub/ptrs-ukb/output/data_split/' + prefix + str(i) + '.txt')\n",
    "\n",
    "clump_file = '/vol/bmd/yanyul/UKB/ld_clump/gwas_clump_x_subset' + str(i) + '_x_' + trait + '.clumped_snp'\n",
    "gwas_file = '/vol/bmd/yanyul/UKB/gwas_on_subset/gwas_runs_in_tsv/gwas_in_tsv_subset' + str(i) + '_x_' + trait + '.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-23 16:40:21 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'f0' as type 'str' (type not specified)\n",
      "  Loading column 'f1' as type 'str' (type not specified)\n",
      "\n",
      "2019-12-23 16:40:21 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "  Loading column 'minor_allele' as type 'str' (type not specified)\n",
      "  Loading column 'minor_AF' as type 'str' (type not specified)\n",
      "  Loading column 'low_confidence_variant' as type 'str' (type not specified)\n",
      "  Loading column 'n_complete_samples' as type 'str' (type not specified)\n",
      "  Loading column 'AC' as type 'str' (type not specified)\n",
      "  Loading column 'ytx' as type 'str' (type not specified)\n",
      "  Loading column 'beta' as type 'float64' (user-specified)\n",
      "  Loading column 'se' as type 'float64' (user-specified)\n",
      "  Loading column 'tstat' as type 'str' (type not specified)\n",
      "  Loading column 'pval' as type 'float64' (user-specified)\n",
      "  Loading column 'rsid' as type 'str' (type not specified)\n",
      "  Loading column 'ref' as type 'str' (type not specified)\n",
      "  Loading column 'alt' as type 'str' (type not specified)\n",
      "\n",
      "2019-12-23 16:40:22 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'f0' as type 'str' (type not specified)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ht_indiv = hl.import_table(indiv_files, key = ['f0'], no_header = True, delimiter = ' ')\n",
    "gwas_tsv = hl.import_table(gwas_file, key = ['rsid'], types = {'beta' : hl.tfloat, 'se' : hl.tfloat, 'pval' : hl.tfloat})\n",
    "clump_snp = hl.import_table(clump_file, key = ['f0'], no_header = True)\n",
    "# gwas_tsv = gwas_tsv.annotate(is_clump = clump_snp[gwas_tsv.rsid])\n",
    "gwas_tsv = gwas_tsv.filter(hl.is_defined(clump_snp[gwas_tsv.rsid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = hl.parse_variant(gwas_tsv.variant)\n",
    "gwas_tsv = gwas_tsv.annotate(**k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_tsv = gwas_tsv.key_by(gwas_tsv.locus, gwas_tsv.alleles)\n",
    "# gwas_tsv = gwas_tsv.repartition(gwas_tsv.n_partitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-23 16:40:25 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-23 16:40:26 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-23 16:40:52 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    }
   ],
   "source": [
    "gwas_tsv = gwas_tsv.repartition(40)\n",
    "gwas_tsv = gwas_tsv.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_this = mt.filter_rows(hl.is_defined(gwas_tsv[mt.locus, mt.alleles]))\n",
    "mt_this = mt_this.filter_cols(hl.is_defined(ht_indiv[mt_this.s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_this = mt_this.repartition(100)\n",
    "mt_this = mt_this.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29543, 18950)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_this.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gwas_annot = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_this = mt_this.annotate_rows(\n",
    "    gwas_beta = gwas_tsv[mt_this.locus, mt_this.alleles].beta, \n",
    "    gwas_pval = gwas_tsv[mt_this.locus, mt_this.alleles].pval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_thresholds = [ 5e-8, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.05, 0.1, 0.5, 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prs = {\n",
    "    'pval_thres_' + str(i) : hl.agg.sum(mt_this.gwas_beta * mt_this.dosage * hl.int(mt_this.gwas_pval < i)) for i in pval_thresholds\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_this = mt_this.annotate_cols(**prs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_this = mt_this.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead style=\"font-weight: bold;\"><tr><td>s</td><td>pval_thres_5e-08</td></tr>\n",
       "<tr><td>str</td><td>float64</td></tr>\n",
       "</thead><tbody><tr><td>&quot;5595764&quot;</td><td>5.88e-01</td></tr>\n",
       "<tr><td>&quot;5172041&quot;</td><td>3.82e-01</td></tr>\n",
       "<tr><td>&quot;2266650&quot;</td><td>-2.10e-01</td></tr>\n",
       "<tr><td>&quot;4860485&quot;</td><td>6.71e-01</td></tr>\n",
       "<tr><td>&quot;3131191&quot;</td><td>1.20e+00</td></tr>\n",
       "<tr><td>&quot;1528789&quot;</td><td>7.58e-03</td></tr>\n",
       "<tr><td>&quot;3197109&quot;</td><td>3.44e-03</td></tr>\n",
       "<tr><td>&quot;5008884&quot;</td><td>2.61e-01</td></tr>\n",
       "<tr><td>&quot;1517276&quot;</td><td>8.51e-01</td></tr>\n",
       "<tr><td>&quot;2468738&quot;</td><td>4.63e-01</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+-----------+------------------+\n",
       "| s         | pval_thres_5e-08 |\n",
       "+-----------+------------------+\n",
       "| str       |          float64 |\n",
       "+-----------+------------------+\n",
       "| \"5595764\" |         5.88e-01 |\n",
       "| \"5172041\" |         3.82e-01 |\n",
       "| \"2266650\" |        -2.10e-01 |\n",
       "| \"4860485\" |         6.71e-01 |\n",
       "| \"3131191\" |         1.20e+00 |\n",
       "| \"1528789\" |         7.58e-03 |\n",
       "| \"3197109\" |         3.44e-03 |\n",
       "| \"5008884\" |         2.61e-01 |\n",
       "| \"1517276\" |         8.51e-01 |\n",
       "| \"2468738\" |         4.63e-01 |\n",
       "+-----------+------------------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mt_this['pval_thres_5e-08'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_clump = open(clump_file, 'r')\n",
    "l_clump = f_clump.readlines()\n",
    "f_clump.close()\n",
    "dic_clump = {}\n",
    "for i in l_clump:\n",
    "    dic_clump[i.strip()] = 1\n",
    "out = []\n",
    "with open(gwas_file, 'r') as f:\n",
    "    header = next(f)\n",
    "    out.append(header)\n",
    "    rsid_idx = header.split('\\t').index('rsid')\n",
    "    for i in f:\n",
    "        k = i.split('\\t')[rsid_idx]\n",
    "        if k in dic_clump:\n",
    "            out.append(i)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# d = pd.read_csv('\\n'.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2007626"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = hl.read_matrix_table('/vol/bmd/yanyul/UKB/tmp/test_prs_x_test_subset1_x_wbc.prs.ht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-24 08:59:32 Hail: INFO: merging 2 files totalling 869.1K...\n",
      "2019-12-24 08:59:32 Hail: INFO: while writing:\n",
      "    test.tsv.bgz\n",
      "  merge time: 29.500ms\n"
     ]
    }
   ],
   "source": [
    "f.col.export('test.tsv.bgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\tpval_thres_5e-08\tpval_thres_1e-07\tpval_thres_1e-06\tpval_thres_1e-05\tpval_thres_0.0001\tpval_thres_0.001\tpval_thres_0.01\tpval_thres_0.1\tpval_thres_0.05\tpval_thres_0.5\tpval_thres_1.0\r\n",
      "5595764\t9.6724e-02\t9.6724e-02\t3.0077e-02\t7.6352e-03\t2.0318e-01\t3.6196e-01\t5.2011e-01\t2.2488e-01\t-7.5449e-01\t5.0274e-01\t5.9248e-01\r\n",
      "5172041\t-5.7805e-04\t-5.7805e-04\t7.5673e-02\t9.2538e-02\t2.6782e-01\t6.0770e-01\t5.2760e-01\t2.8239e-01\t-7.1257e-01\t5.8479e-01\t5.0037e-01\r\n",
      "2266650\t-1.4736e-03\t-1.4736e-03\t-5.6721e-02\t-8.9969e-03\t4.4957e-02\t3.3760e-01\t2.1838e-01\t3.6890e-01\t-5.9017e-01\t4.7468e-01\t3.4717e-01\r\n",
      "4860485\t2.4917e-02\t2.4917e-02\t-1.5172e-02\t-9.5946e-03\t9.8818e-02\t5.4127e-01\t6.9360e-01\t-3.0089e-01\t-8.6135e-01\t2.9628e-01\t2.8129e-01\r\n",
      "3131191\t3.3175e-02\t3.3175e-02\t6.0356e-02\t1.1818e-02\t-9.2583e-03\t1.4733e-01\t1.4158e-01\t-7.9820e-01\t-1.5161e+00\t-6.9655e-01\t-8.3469e-01\r\n",
      "1528789\t2.3629e-02\t2.3629e-02\t4.8931e-02\t-6.1449e-02\t1.0919e-01\t4.8954e-01\t1.4597e+00\t3.2050e+00\t1.8402e+00\t4.1675e+00\t4.0017e+00\r\n",
      "3197109\t-4.3886e-02\t-4.3886e-02\t-7.3474e-02\t-1.9592e-01\t-1.7730e-01\t-2.7448e-01\t-1.3777e-01\t-4.0395e-01\t-1.1953e+00\t-1.6322e-01\t-2.7241e-01\r\n",
      "5008884\t5.3701e-02\t5.3701e-02\t7.0706e-02\t1.0896e-01\t1.6477e-01\t2.7881e-01\t-1.1192e-01\t-8.7737e-01\t-1.2946e+00\t-2.5153e-01\t-3.0981e-01\r\n",
      "1517276\t-1.2128e-02\t-1.2128e-02\t-2.2161e-02\t1.0366e-02\t4.6169e-02\t-7.0043e-03\t6.7356e-03\t-6.6713e-02\t-9.0370e-01\t1.6620e-01\t3.6093e-02\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!zcat test.tsv.bgz|head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
