{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running on Apache Spark version 2.4.1\n",
      "SparkUI available at http://nucleus.cels.anl.gov:4040\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.28-61941242c15d\n",
      "LOGGING: writing to /vol/bmd/yanyul/GitHub/ptrs-ukb/notebook/hail-20191223-1629-0.2.28-61941242c15d.log\n"
     ]
    }
   ],
   "source": [
    "import hail as hl\n",
    "hl.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-23 16:29:43 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "  Loading column 'minor_allele' as type 'str' (type not specified)\n",
      "  Loading column 'minor_AF' as type 'str' (type not specified)\n",
      "  Loading column 'low_confidence_variant' as type 'str' (type not specified)\n",
      "  Loading column 'n_complete_samples' as type 'str' (type not specified)\n",
      "  Loading column 'AC' as type 'str' (type not specified)\n",
      "  Loading column 'ytx' as type 'str' (type not specified)\n",
      "  Loading column 'beta' as type 'str' (type not specified)\n",
      "  Loading column 'se' as type 'str' (type not specified)\n",
      "  Loading column 'tstat' as type 'str' (type not specified)\n",
      "  Loading column 'pval' as type 'str' (type not specified)\n",
      "  Loading column 'rsid' as type 'str' (type not specified)\n",
      "  Loading column 'ref' as type 'str' (type not specified)\n",
      "  Loading column 'alt' as type 'str' (type not specified)\n",
      "\n",
      "2019-12-23 16:29:44 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'f0' as type 'str' (type not specified)\n",
      "\n",
      "2019-12-23 16:29:45 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-12-23 16:29:45 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-12-23 16:29:45 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-12-23 16:29:48 Hail: INFO: Number of BGEN files parsed: 1\n",
      "2019-12-23 16:29:48 Hail: INFO: Number of samples in BGEN files: 487409\n",
      "2019-12-23 16:29:48 Hail: INFO: Number of variants across all BGEN files: 1255683\n",
      "2019-12-23 16:30:04 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-23 16:31:06 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-23 16:40:21 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    }
   ],
   "source": [
    "gwas_sample = hl.import_table(\n",
    "    '/vol/bmd/yanyul/UKB/gwas_on_subset/gwas_runs_in_tsv/gwas_in_tsv_subset13_x_height.tsv',\n",
    "    key = ['rsid', 'variant']\n",
    ")\n",
    "f = pd.read_csv('../external_data/martin_et_al_2019ng_table_s6_trait_description.tsv', sep = '\\t')\n",
    "traits = [ i.lower() for i in f['short'].to_list() ]\n",
    "clumped_snp_files = []\n",
    "for i in traits:\n",
    "    for j in range(1, 18):\n",
    "        clumped_snp_files.append('/vol/bmd/yanyul/UKB/ld_clump/gwas_clump_x_subset' + str(j) + '_x_' + i + '.clumped_snp')\n",
    "ht_var = hl.import_table(clumped_snp_files, no_header = True, key = 'f0')\n",
    "ht_var = ht_var.key_by('f0')\n",
    "gwas_sample = gwas_sample.annotate(is_clump_var = ht_var[gwas_sample.rsid])\n",
    "ht_var = gwas_sample.filter(hl.is_defined(gwas_sample.is_clump_var))\n",
    "chr_all = '{' + ','.join([str(i) for i in range(22, 23)]) + '}'\n",
    "index_dic = {\n",
    "    f'/vol/bmd/data/ukbiobank/genotypes/v3/ukb_imp_chr{i}_v3.bgen' : f'/vol/bmd/yanyul/UKB/bgen_idx/ukb_imp_chr{i}_v3.bgen.idx2' for i in range(1, 23)\n",
    "}\n",
    "mt = hl.import_bgen(\n",
    "    '/vol/bmd/data/ukbiobank/genotypes/v3/ukb_imp_chr' + chr_all + '_v3.bgen', \n",
    "    index_file_map = index_dic,\n",
    "    entry_fields = ['dosage'],\n",
    "    sample_file = '/vol/bmd/data/ukbiobank/genotypes/v3/ukb19526_imp_chr1_v3_s487395.sample',\n",
    "    variants = hl.parse_variant(ht_var.variant)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mt.count_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_files = []\n",
    "for i in ['African', 'Chinese', 'Indian']:\n",
    "    indiv_files.append('/vol/bmd/yanyul/GitHub/ptrs-ukb/output/data_split/' + i + '.txt')\n",
    "\n",
    "# for subset i\n",
    "# for trait height\n",
    "i = 1\n",
    "trait = 'height'\n",
    "\n",
    "for prefix in [ 'British-test-', 'British-validation-' ]:\n",
    "    indiv_files.append('/vol/bmd/yanyul/GitHub/ptrs-ukb/output/data_split/' + prefix + str(i) + '.txt')\n",
    "    indiv_files.append('/vol/bmd/yanyul/GitHub/ptrs-ukb/output/data_split/' + prefix + str(i) + '.txt')\n",
    "\n",
    "clump_file = '/vol/bmd/yanyul/UKB/ld_clump/gwas_clump_x_subset' + str(i) + '_x_' + trait + '.clumped_snp'\n",
    "gwas_file = '/vol/bmd/yanyul/UKB/gwas_on_subset/gwas_runs_in_tsv/gwas_in_tsv_subset' + str(i) + '_x_' + trait + '.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-23 16:40:21 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'f0' as type 'str' (type not specified)\n",
      "  Loading column 'f1' as type 'str' (type not specified)\n",
      "\n",
      "2019-12-23 16:40:21 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'variant' as type 'str' (type not specified)\n",
      "  Loading column 'minor_allele' as type 'str' (type not specified)\n",
      "  Loading column 'minor_AF' as type 'str' (type not specified)\n",
      "  Loading column 'low_confidence_variant' as type 'str' (type not specified)\n",
      "  Loading column 'n_complete_samples' as type 'str' (type not specified)\n",
      "  Loading column 'AC' as type 'str' (type not specified)\n",
      "  Loading column 'ytx' as type 'str' (type not specified)\n",
      "  Loading column 'beta' as type 'float64' (user-specified)\n",
      "  Loading column 'se' as type 'float64' (user-specified)\n",
      "  Loading column 'tstat' as type 'str' (type not specified)\n",
      "  Loading column 'pval' as type 'float64' (user-specified)\n",
      "  Loading column 'rsid' as type 'str' (type not specified)\n",
      "  Loading column 'ref' as type 'str' (type not specified)\n",
      "  Loading column 'alt' as type 'str' (type not specified)\n",
      "\n",
      "2019-12-23 16:40:22 Hail: INFO: Reading table with no type imputation\n",
      "  Loading column 'f0' as type 'str' (type not specified)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ht_indiv = hl.import_table(indiv_files, key = ['f0'], no_header = True, delimiter = ' ')\n",
    "gwas_tsv = hl.import_table(gwas_file, key = ['rsid'], types = {'beta' : hl.tfloat, 'se' : hl.tfloat, 'pval' : hl.tfloat})\n",
    "clump_snp = hl.import_table(clump_file, key = ['f0'], no_header = True)\n",
    "# gwas_tsv = gwas_tsv.annotate(is_clump = clump_snp[gwas_tsv.rsid])\n",
    "gwas_tsv = gwas_tsv.filter(hl.is_defined(clump_snp[gwas_tsv.rsid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = hl.parse_variant(gwas_tsv.variant)\n",
    "gwas_tsv = gwas_tsv.annotate(**k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_tsv = gwas_tsv.key_by(gwas_tsv.locus, gwas_tsv.alleles)\n",
    "# gwas_tsv = gwas_tsv.repartition(gwas_tsv.n_partitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-23 16:40:25 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-23 16:40:26 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-12-23 16:40:52 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    }
   ],
   "source": [
    "gwas_tsv = gwas_tsv.repartition(40)\n",
    "gwas_tsv = gwas_tsv.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_this = mt.filter_rows(hl.is_defined(gwas_tsv[mt.locus, mt.alleles]))\n",
    "mt_this = mt_this.filter_cols(hl.is_defined(ht_indiv[mt_this.s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_this = mt_this.repartition(100)\n",
    "mt_this = mt_this.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29543, 18950)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_this.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gwas_annot = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_this = mt_this.annotate_rows(\n",
    "    gwas_beta = gwas_tsv[mt_this.locus, mt_this.alleles].beta, \n",
    "    gwas_pval = gwas_tsv[mt_this.locus, mt_this.alleles].pval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pval_thresholds = [ 5e-8, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.05, 0.1, 0.5, 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prs = {\n",
    "    'pval_thres_' + str(i) : hl.agg.sum(mt_this.gwas_beta * mt_this.dosage * hl.int(mt_this.gwas_pval < i)) for i in pval_thresholds\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_this = mt_this.annotate_cols(**prs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_this = mt_this.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead style=\"font-weight: bold;\"><tr><td>s</td><td>pval_thres_5e-08</td></tr>\n",
       "<tr><td>str</td><td>float64</td></tr>\n",
       "</thead><tbody><tr><td>&quot;5595764&quot;</td><td>5.88e-01</td></tr>\n",
       "<tr><td>&quot;5172041&quot;</td><td>3.82e-01</td></tr>\n",
       "<tr><td>&quot;2266650&quot;</td><td>-2.10e-01</td></tr>\n",
       "<tr><td>&quot;4860485&quot;</td><td>6.71e-01</td></tr>\n",
       "<tr><td>&quot;3131191&quot;</td><td>1.20e+00</td></tr>\n",
       "<tr><td>&quot;1528789&quot;</td><td>7.58e-03</td></tr>\n",
       "<tr><td>&quot;3197109&quot;</td><td>3.44e-03</td></tr>\n",
       "<tr><td>&quot;5008884&quot;</td><td>2.61e-01</td></tr>\n",
       "<tr><td>&quot;1517276&quot;</td><td>8.51e-01</td></tr>\n",
       "<tr><td>&quot;2468738&quot;</td><td>4.63e-01</td></tr>\n",
       "</tbody></table><p style=\"background: #fdd; padding: 0.4em;\">showing top 10 rows</p>\n"
      ],
      "text/plain": [
       "+-----------+------------------+\n",
       "| s         | pval_thres_5e-08 |\n",
       "+-----------+------------------+\n",
       "| str       |          float64 |\n",
       "+-----------+------------------+\n",
       "| \"5595764\" |         5.88e-01 |\n",
       "| \"5172041\" |         3.82e-01 |\n",
       "| \"2266650\" |        -2.10e-01 |\n",
       "| \"4860485\" |         6.71e-01 |\n",
       "| \"3131191\" |         1.20e+00 |\n",
       "| \"1528789\" |         7.58e-03 |\n",
       "| \"3197109\" |         3.44e-03 |\n",
       "| \"5008884\" |         2.61e-01 |\n",
       "| \"1517276\" |         8.51e-01 |\n",
       "| \"2468738\" |         4.63e-01 |\n",
       "+-----------+------------------+\n",
       "showing top 10 rows"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mt_this['pval_thres_5e-08'].show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
